<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://example.com/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main">
  
    <article id="post-5" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/18/5/" class="article-date">
  <time class="dt-published" datetime="2023-03-17T16:26:22.000Z" itemprop="datePublished">2023-03-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2023/03/18/5/">5</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>create model</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/MengYa_Dream/article/details/126690336-%E4%B8%BB%E8%A6%81%E5%8F%82%E8%80%83">https://blog.csdn.net/MengYa_Dream/article/details/126690336-主要参考</a></p>
<p>The model architectures included come from a wide variety of sources. Sources, including papers, original impl (“reference code”) that Ross rewrote / adapted, and PyTorch impl that he leveraged directly (“code”) are listed below.</p>
<p>Most included models have pretrained weights. The weights are either:</p>
<p>from their original sources<br>ported by myself from their original impl in a different framework (e.g. Tensorflow models)<br>trained from scratch using the included training script<br>The validation results for the pretrained weights can be found here.</p>
<p>最近一年 Vision Transformer 及其相关改进的工作层出不穷，在他们开源的代码中，大部分都用到了这样一个库：timm。各位炼丹师应该已经想必已经对其无比熟悉了，本文将介绍其中最关键的函数之一：create_model 函数</p>
<p>create_model 函数是用来创建一个网络模型（如 ResNet、ViT 等），timm 库本身可供直接调用的模型已有接近400个，用户也可以自己实现一些模型并注册进 timm （这一部分内容将在下一小节着重介绍），供自己调用</p>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><h2 id="create-model"><a href="#create-model" class="headerlink" title="create_model"></a>create_model</h2><p>该<code>create_model</code>函数是用来在里面创建数百个模型的<code>timm</code>。它还期望一堆<code>**kwargs</code>诸如<code>features_only</code>和<code>out_indices</code>并将这两个传递<code>**kwargs</code>给<code>create_model</code>函数来创建一个特征提取器。让我们看看如何？</p>
<p>该<code>create_model</code>函数本身只有大约 50 行代码。所以所有的神奇的事情都必须在其他地方发生。您可能已经知道，其中的每个模型名称<code>timm.list_models()</code>实际上都是一个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model_name:模型的名字</span></span><br><span class="line"><span class="comment">#pretrained:是否加载预训练模型</span></span><br><span class="line"><span class="comment">#checkpoint_path:加载预训练模型的路径</span></span><br><span class="line"><span class="comment">#scriptable:是否使用脚本化</span></span><br><span class="line"><span class="comment">#exportable:是否使用导出</span></span><br><span class="line"><span class="comment">#no_jit:是否使用jit</span></span><br><span class="line"><span class="comment">#**kwargs:其他参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#首先通过split_model_name函数将模型名字分割成两部分，一部分是模型的来源，一部分是模型的名字</span></span><br><span class="line"><span class="comment">#然后通过is_model函数判断模型是否存在</span></span><br><span class="line"><span class="comment">#如果存在，通过model_entrypoint函数获取模型的入口函数</span></span><br><span class="line"><span class="comment">#然后通过set_layer_config函数设置层的配置</span></span><br><span class="line"><span class="comment">#最后通过load_checkpoint函数加载预训练模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        model_name,</span></span></span><br><span class="line"><span class="params"><span class="function">        pretrained=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        checkpoint_path=<span class="string">&#x27;&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        scriptable=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        exportable=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        no_jit=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        **kwargs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model_name (str): name of model to instantiate</span></span><br><span class="line"><span class="string">        pretrained (bool): load pretrained ImageNet-1k weights if true</span></span><br><span class="line"><span class="string">        checkpoint_path (str): path of checkpoint to load after model is initialized</span></span><br><span class="line"><span class="string">        scriptable (bool): set layer config so that model is jit scriptable (not working for all models yet)</span></span><br><span class="line"><span class="string">        exportable (bool): set layer config so that model is traceable / ONNX exportable (not fully impl/obeyed yet)</span></span><br><span class="line"><span class="string">        no_jit (bool): set layer config so that model doesn&#x27;t utilize jit scripted layers (so far activations only)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Keyword Args:</span></span><br><span class="line"><span class="string">        drop_rate (float): dropout rate for training (default: 0.0)</span></span><br><span class="line"><span class="string">        global_pool (str): global pool type (default: &#x27;avg&#x27;)</span></span><br><span class="line"><span class="string">        **: other kwargs are model specific</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    source_name, model_name = split_model_name(model_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># handle backwards compat with drop_connect -&gt; drop_path change</span></span><br><span class="line">    drop_connect_rate = kwargs.pop(<span class="string">&#x27;drop_connect_rate&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> drop_connect_rate <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> kwargs.get(<span class="string">&#x27;drop_path_rate&#x27;</span>, <span class="literal">None</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;WARNING: &#x27;drop_connect&#x27; as an argument is deprecated, please use &#x27;drop_path&#x27;.&quot;</span></span><br><span class="line">              <span class="string">&quot; Setting drop_path to %f.&quot;</span> % drop_connect_rate)</span><br><span class="line">        kwargs[<span class="string">&#x27;drop_path_rate&#x27;</span>] = drop_connect_rate</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parameters that aren&#x27;t supported by all models or are intended to only override model defaults if set</span></span><br><span class="line">    <span class="comment"># should default to None in command line args/cfg. Remove them if they are present and not set so that</span></span><br><span class="line">    <span class="comment"># non-supporting models don&#x27;t break and default args remain in effect.</span></span><br><span class="line">    kwargs = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items() <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> source_name == <span class="string">&#x27;hf_hub&#x27;</span>:</span><br><span class="line">        <span class="comment"># For model names specified in the form `hf_hub:path/architecture_name#revision`,</span></span><br><span class="line">        <span class="comment"># load model weights + default_cfg from Hugging Face hub.</span></span><br><span class="line">        hf_default_cfg, model_name = load_model_config_from_hf(model_name)</span><br><span class="line">        kwargs[<span class="string">&#x27;external_default_cfg&#x27;</span>] = hf_default_cfg  <span class="comment"># FIXME revamp default_cfg interface someday</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_model(model_name):</span><br><span class="line">        create_fn = model_entrypoint(model_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;Unknown model (%s)&#x27;</span> % model_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> set_layer_config(scriptable=scriptable, exportable=exportable, no_jit=no_jit):</span><br><span class="line">        model = create_fn(pretrained=pretrained, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> checkpoint_path:</span><br><span class="line">        load_checkpoint(model, checkpoint_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>



<h2 id="timm-models-registry模块"><a href="#timm-models-registry模块" class="headerlink" title="timm.models.registry模块"></a>timm.models.registry模块</h2><p>代码中有这样一行</p>
<p>create_fn = model_entrypoint(model_name)</p>
<p>函数获取模型的入口函数</p>
<p>那么他是如何实现的呢，他的运行主要依赖于timm.models.registry模块</p>
<p>有一个字典_model_entrypoints = {}  # mapping of model names to entrypoint fns</p>
<p>包含了所有的模型名称和他们各自的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_model</span>(<span class="params">model_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Check if a model name exists</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> model_name <span class="keyword">in</span> _model_entrypoints</span><br></pre></td></tr></table></figure>

<p>model_entrypoint 函数从 <code>_model_entrypoints</code> 内部得到模型的构造函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_entrypoint</span>(<span class="params">model_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Fetch a model entrypoint for specified model name</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> _model_entrypoints[model_name]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(timm.models.registry._model_entrypoints)</span><br><span class="line"><span class="built_in">print</span>(timm.models.registry._model_entrypoints.keys())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">输出是下面这样子的，太多了，只贴出来一点</span></span><br><span class="line"><span class="string">&#123;&#x27;vit_tiny_patch16_224&#x27;: &lt;function vit_tiny_patch16_224 at 0x000002450498A048&gt;, &#x27;vit_tiny_patch16_384&#x27;: &lt;function vit_tiny_patch16_384 at 0x000002450498A0D0&gt;, &#x27;vit_small_patch32_224&#x27;: &lt;function vit_small_patch32_224 at 0x000002450498A158&gt;, &#x27;vit_small_patch32_384&#x27;: &lt;function vit_small_patch32_384 at 0x000002450498A1E0&gt;&#125;</span></span><br><span class="line"><span class="string">可以看到是个字典，key是模型名称，value是模型的构造函数，</span></span><br><span class="line"><span class="string">以vit为例，vit_base_patch16_224模型的构造函数，他存在于timm.models.vision_trasnformer.py</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">create_fn = model_entrypoint(model_name)</span></span><br><span class="line"><span class="string">#以model_name =  &#x27;xception71&#x27; 为例</span></span><br><span class="line"><span class="string">#&lt;function xception71 at 0x7fc0cba0eca0&gt;</span></span><br><span class="line"><span class="string">#&lt;function timm.models.xception_aligned.xception71(pretrained=False, **kwargs)&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="timm-models-vision-trasnformer模块"><a href="#timm-models-vision-trasnformer模块" class="headerlink" title="timm.models.vision_trasnformer模块"></a>timm.models.vision_trasnformer模块</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@register_model</span><br><span class="line">def vit_base_patch16_224(pretrained=False, **kwargs):</span><br><span class="line">    &quot;&quot;&quot; ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).</span><br><span class="line">    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)</span><br><span class="line">    model = _create_vision_transformer(&#x27;vit_base_patch16_224&#x27;, pretrained=pretrained, **model_kwargs)</span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>

<p>注释写了出处，模型的原论文</p>
<p>model_kwargs把我这个模型的独特参数和传进来的**kwargs一起打包成参数字典，传给<code>_create_vision_transformer</code>函数</p>
<p>这个函数写在文件的开头，vit的模型都是他生成的，区别就是架构参数不一样，比如层数，注意力头数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_create_vision_transformer</span>(<span class="params">variant, pretrained=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">if</span> kwargs.get(<span class="string">&#x27;features_only&#x27;</span>, <span class="literal">None</span>):</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;features_only not implemented for Vision Transformer models.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;flexi&#x27;</span> <span class="keyword">in</span> variant:</span><br><span class="line">        <span class="comment"># FIXME Google FlexiViT pretrained models have a strong preference for bilinear patch / embed</span></span><br><span class="line">        <span class="comment"># interpolation, other pretrained models resize better w/ anti-aliased bicubic interpolation.</span></span><br><span class="line">        _filter_fn = partial(checkpoint_filter_fn, interpolation=<span class="string">&#x27;bilinear&#x27;</span>, antialias=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        _filter_fn = checkpoint_filter_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> build_model_with_cfg(</span><br><span class="line">        VisionTransformer, variant, pretrained,</span><br><span class="line">        pretrained_filter_fn=_filter_fn,</span><br><span class="line">        **kwargs,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>挖坑提高，星星是什么</p>
<p>variant 变种，就是模型的名字，比如在vit类上生成的vit-base</p>
<p>函数之中，会再调用 build_model_with_cfg 函数并将一个构造器类 VisionTransformer、变量名 resnet34、一个 default_cfg 和一些 **kwargs 传入其中。</p>
<p> **kwargs就是之前打包的模型参数和  **kwargs</p>
<h2 id="build-model-with-cfg"><a href="#build-model-with-cfg" class="headerlink" title="build_model_with_cfg"></a>build_model_with_cfg</h2><p>这个 build_model_with_cfg 函数负责：</p>
<ul>
<li>真正地实例化一个模型类来创建一个模型</li>
<li>若 pruned=True，对模型进行剪枝</li>
<li>若 pretrained=True，加载预训练模型参数</li>
<li>若 features_only=True，将模型转换为特征提取器</li>
</ul>
<h2 id="register-model装饰器"><a href="#register-model装饰器" class="headerlink" title="register_model装饰器"></a>register_model装饰器</h2><p>以上就是读者在使用 timm 库时的基本方法，其实到这里你应该已经能够使用它训练自己的分类模型了。但是如果还想进一步搞清楚它的框架原理，并在它的基础上做修改，本节可能会帮到你。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361837010">https://zhuanlan.zhihu.com/p/361837010</a></p>
<p>但是问题来了，这个字典怎么来的，我写好代码之后一个一个添加进去的吗？（也不是不行hh</p>
<p>但是太麻烦，这时候就用到我们的 <code>register_model</code> 装饰器，它可以不断地像其中添加模型名称和它对应的构造函数，一开始的字典是空的，是装饰器自动添加的，其源码如下</p>
<p>2.5 sys.modules<br>该属性是一个字典，包含的是各种已加载的模块的模块名到模块具体位置的映射。</p>
<p>通过手动修改这个字典，可以重新加载某些模块；但要注意，切记不要大意删除了一些基本的项，否则可能会导致 Python 整个儿无法运行。</p>
<p>mod = sys.modules[fn.__module__]</p>
<p>这里我print一下fn.__module__ 应该是xxx.xxx.vit_base</p>
<p>模型名称就是vit_base</p>
<pre><code>module_name_split = fn.__module__.split(&#39;.&#39;)
module_name = module_name_split[-1] if len(module_name_split) else &#39;&#39;
</code></pre>
<p>还是打印一下，我有点分不清module_name和model_name</p>
<p>  _model_to_module[model_name] = module_name</p>
<p>  _module_to_models[module_name].add(model_name)</p>
<details>
  <summary>展开查看</summary>
  <pre><code>
def register_model(fn):
    # lookup containing module
    mod = sys.modules[fn.__module__]
    module_name_split = fn.__module__.split('.')
    module_name = module_name_split[-1] if len(module_name_split) else ''
    # add model to __all__ in module
    model_name = fn.__name__
    if hasattr(mod, '__all__'):
        mod.__all__.append(model_name)
    else:
        mod.__all__ = [model_name]
    # add entries to registry dict/sets
    _model_entrypoints[model_name] = fn
    _model_to_module[model_name] = module_name
    _module_to_models[module_name].add(model_name)
    has_pretrained = False  # check if model has a pretrained url to allow filtering on this
    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:
        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing
        # entrypoints or non-matching combos
        has_pretrained = 'url' in mod.default_cfgs[model_name] and 'http' in mod.default_cfgs[model_name]['url']
        _model_default_cfgs[model_name] = deepcopy(mod.default_cfgs[model_name])
    if has_pretrained:
        _model_has_pretrained.add(model_name)
    return fn
  </code></pre>
</details>


<p>重点是这一句</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_name = fn.__name__</span><br><span class="line"></span><br><span class="line">_model_entrypoints[model_name] = fn</span><br></pre></td></tr></table></figure>

<p>它将给定的 <code>fn</code> 添加到 <code>_model_entrypoints</code> 其键名为 <code>fn.__name__</code></p>
<p>查看模型定义文件</p>
<p>我们会发现 timm 中的每个模型都有一个 @register_model</p>
<p>例如vit-base</p>
<p>代码很简单明了，但是我还是不懂他怎么发生作用的</p>
<p>他是什么时候开始把构造函数加进那个空字典的？</p>
<p>我要自己注册函数应该如何做？</p>
<p>挖坑拓展，import</p>
<h2 id="modelconfig"><a href="#modelconfig" class="headerlink" title="modelconfig"></a>modelconfig</h2><p>timm 中所有的模型都有一个默认的配置，包括指向它的预训练权重参数的URL、类别数、输入图像尺寸、池化尺寸等。</p>
<p>此默认配置与其他参数（如构造函数类和一些模型参数）一起传递给 <code>build_model_with_cfg</code> 函数</p>
<h1 id="下载pretrained"><a href="#下载pretrained" class="headerlink" title="下载pretrained"></a>下载pretrained</h1><h3 id="pretrained"><a href="#pretrained" class="headerlink" title="pretrained"></a>pretrained</h3><p>如果我们传入 <code>pretrained=True</code>，那么 timm 会从对应的 URL 下载模型权重参数并载入模型，只有当第一次（即本地还没有对应模型参数时）会去下载，之后会直接从本地加载模型权重参数。</p>
<p>True:timm直接根据对应的URL下载模型权重参数并加载到模型，注意，只有本地没有对应模型参数时才会下载，也就是说，通常在第一次运行时下载对应模型参数，之后会直接从本地加载模型权重参数。（报错经验：这里容易出现的报错，就是第一次下载时，可能因为网络等原因，下载不完全，因此，有时需要删除对应文件，重新下载）</p>
<p>model = timm.create_model(‘resnet34’, pretrained=True)</p>
<p>Downloading: “<a target="_blank" rel="noopener" href="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth&quot;">https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth&quot;</a> to /home/song/.cache/torch/hub/checkpoints/resnet34-43635321.pth</p>
<h1 id="yuanli"><a href="#yuanli" class="headerlink" title="yuanli"></a>yuanli</h1><h2 id="registry"><a href="#registry" class="headerlink" title="registry"></a>registry</h2><p>在了解了 <code>create_model</code> 函数的基本使用之后，我们来深入探索一下 <code>create_model</code> 函数的源码，看一下究竟是怎样实现从模型到特征提取器的转换的</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44966641/article/details/121364784">https://blog.csdn.net/weixin_44966641/article/details/121364784</a></p>
<p>在 timm 内部，有一个字典称为 <code>_model_entrypoints</code> 包含了所有的模型名称和他们各自的函数。比如说，我们可以通过 <code>model_entrypoint</code> 函数从 <code>_model_entrypoints</code> 内部得到 <code>xception71</code> 模型的构造函数。</p>
<p>如我们所见，在 <code>timm.models.xception_aligned</code> 模块中有一个函数称为 <code>xception71</code> 。类似的，timm 中的每一个模型都有着一个这样的构造函数。事实上，内部的 <code>_model_entrypoints</code> 字典大概长这个样子：</p>
<details style='background-color:#f9f2f4'> <summary><font color='#c7254e' size='3px'> view code</font></summary>  不适合放代码，都锁在一起了  </details>


<p>在 timm 对应的模块中，每个模型都有一个构造器。比如说 ResNets 系列模型被定义在 <code>timm.models.resnet</code> 模块中。因此，实际上我们有两种方式来创建一个 <code>resnet34</code> 模型</p>
<p>import timm from timm.models.resnet import resnet34 <em># 使用 create_model</em> m = timm.create_model(‘resnet34’) <em># 直接调用构造函数</em> m = resnet34()</p>
<p>但使用上，我们无须调用构造函数。所用模型都可以通过 <code>create_model</code> 函数来将创建。</p>
<h1 id="画图，话一个相互之间构成联系的图"><a href="#画图，话一个相互之间构成联系的图" class="headerlink" title="画图，话一个相互之间构成联系的图"></a>画图，话一个相互之间构成联系的图</h1><p>whiteboard</p>
<p>boardmix</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/18/5/" data-id="clfcr7mm40001c4u56cazfp0n" data-title="5" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-5.model" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2023/03/18/5.model/" class="article-date">
  <time class="dt-published" datetime="2023-03-17T16:23:43.348Z" itemprop="datePublished">2023-03-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>create model</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/MengYa_Dream/article/details/126690336-%E4%B8%BB%E8%A6%81%E5%8F%82%E8%80%83">https://blog.csdn.net/MengYa_Dream/article/details/126690336-主要参考</a></p>
<p>The model architectures included come from a wide variety of sources. Sources, including papers, original impl (“reference code”) that Ross rewrote / adapted, and PyTorch impl that he leveraged directly (“code”) are listed below.</p>
<p>Most included models have pretrained weights. The weights are either:</p>
<p>from their original sources<br>ported by myself from their original impl in a different framework (e.g. Tensorflow models)<br>trained from scratch using the included training script<br>The validation results for the pretrained weights can be found here.</p>
<p>最近一年 Vision Transformer 及其相关改进的工作层出不穷，在他们开源的代码中，大部分都用到了这样一个库：timm。各位炼丹师应该已经想必已经对其无比熟悉了，本文将介绍其中最关键的函数之一：create_model 函数</p>
<p>create_model 函数是用来创建一个网络模型（如 ResNet、ViT 等），timm 库本身可供直接调用的模型已有接近400个，用户也可以自己实现一些模型并注册进 timm （这一部分内容将在下一小节着重介绍），供自己调用</p>
<h1 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h1><h2 id="create-model"><a href="#create-model" class="headerlink" title="create_model"></a>create_model</h2><p>该<code>create_model</code>函数是用来在里面创建数百个模型的<code>timm</code>。它还期望一堆<code>**kwargs</code>诸如<code>features_only</code>和<code>out_indices</code>并将这两个传递<code>**kwargs</code>给<code>create_model</code>函数来创建一个特征提取器。让我们看看如何？</p>
<p>该<code>create_model</code>函数本身只有大约 50 行代码。所以所有的神奇的事情都必须在其他地方发生。您可能已经知道，其中的每个模型名称<code>timm.list_models()</code>实际上都是一个函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model_name:模型的名字</span></span><br><span class="line"><span class="comment">#pretrained:是否加载预训练模型</span></span><br><span class="line"><span class="comment">#checkpoint_path:加载预训练模型的路径</span></span><br><span class="line"><span class="comment">#scriptable:是否使用脚本化</span></span><br><span class="line"><span class="comment">#exportable:是否使用导出</span></span><br><span class="line"><span class="comment">#no_jit:是否使用jit</span></span><br><span class="line"><span class="comment">#**kwargs:其他参数</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#首先通过split_model_name函数将模型名字分割成两部分，一部分是模型的来源，一部分是模型的名字</span></span><br><span class="line"><span class="comment">#然后通过is_model函数判断模型是否存在</span></span><br><span class="line"><span class="comment">#如果存在，通过model_entrypoint函数获取模型的入口函数</span></span><br><span class="line"><span class="comment">#然后通过set_layer_config函数设置层的配置</span></span><br><span class="line"><span class="comment">#最后通过load_checkpoint函数加载预训练模型</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_model</span>(<span class="params"></span></span></span><br><span class="line"><span class="params"><span class="function">        model_name,</span></span></span><br><span class="line"><span class="params"><span class="function">        pretrained=<span class="literal">False</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        checkpoint_path=<span class="string">&#x27;&#x27;</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        scriptable=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        exportable=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        no_jit=<span class="literal">None</span>,</span></span></span><br><span class="line"><span class="params"><span class="function">        **kwargs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Create a model</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        model_name (str): name of model to instantiate</span></span><br><span class="line"><span class="string">        pretrained (bool): load pretrained ImageNet-1k weights if true</span></span><br><span class="line"><span class="string">        checkpoint_path (str): path of checkpoint to load after model is initialized</span></span><br><span class="line"><span class="string">        scriptable (bool): set layer config so that model is jit scriptable (not working for all models yet)</span></span><br><span class="line"><span class="string">        exportable (bool): set layer config so that model is traceable / ONNX exportable (not fully impl/obeyed yet)</span></span><br><span class="line"><span class="string">        no_jit (bool): set layer config so that model doesn&#x27;t utilize jit scripted layers (so far activations only)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Keyword Args:</span></span><br><span class="line"><span class="string">        drop_rate (float): dropout rate for training (default: 0.0)</span></span><br><span class="line"><span class="string">        global_pool (str): global pool type (default: &#x27;avg&#x27;)</span></span><br><span class="line"><span class="string">        **: other kwargs are model specific</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    source_name, model_name = split_model_name(model_name)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># handle backwards compat with drop_connect -&gt; drop_path change</span></span><br><span class="line">    drop_connect_rate = kwargs.pop(<span class="string">&#x27;drop_connect_rate&#x27;</span>, <span class="literal">None</span>)</span><br><span class="line">    <span class="keyword">if</span> drop_connect_rate <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> kwargs.get(<span class="string">&#x27;drop_path_rate&#x27;</span>, <span class="literal">None</span>) <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;WARNING: &#x27;drop_connect&#x27; as an argument is deprecated, please use &#x27;drop_path&#x27;.&quot;</span></span><br><span class="line">              <span class="string">&quot; Setting drop_path to %f.&quot;</span> % drop_connect_rate)</span><br><span class="line">        kwargs[<span class="string">&#x27;drop_path_rate&#x27;</span>] = drop_connect_rate</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Parameters that aren&#x27;t supported by all models or are intended to only override model defaults if set</span></span><br><span class="line">    <span class="comment"># should default to None in command line args/cfg. Remove them if they are present and not set so that</span></span><br><span class="line">    <span class="comment"># non-supporting models don&#x27;t break and default args remain in effect.</span></span><br><span class="line">    kwargs = &#123;k: v <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items() <span class="keyword">if</span> v <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>&#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> source_name == <span class="string">&#x27;hf_hub&#x27;</span>:</span><br><span class="line">        <span class="comment"># For model names specified in the form `hf_hub:path/architecture_name#revision`,</span></span><br><span class="line">        <span class="comment"># load model weights + default_cfg from Hugging Face hub.</span></span><br><span class="line">        hf_default_cfg, model_name = load_model_config_from_hf(model_name)</span><br><span class="line">        kwargs[<span class="string">&#x27;external_default_cfg&#x27;</span>] = hf_default_cfg  <span class="comment"># FIXME revamp default_cfg interface someday</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> is_model(model_name):</span><br><span class="line">        create_fn = model_entrypoint(model_name)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;Unknown model (%s)&#x27;</span> % model_name)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> set_layer_config(scriptable=scriptable, exportable=exportable, no_jit=no_jit):</span><br><span class="line">        model = create_fn(pretrained=pretrained, **kwargs)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> checkpoint_path:</span><br><span class="line">        load_checkpoint(model, checkpoint_path)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>



<h2 id="timm-models-registry模块"><a href="#timm-models-registry模块" class="headerlink" title="timm.models.registry模块"></a>timm.models.registry模块</h2><p>代码中有这样一行</p>
<p>create_fn = model_entrypoint(model_name)</p>
<p>函数获取模型的入口函数</p>
<p>那么他是如何实现的呢，他的运行主要依赖于timm.models.registry模块</p>
<p>有一个字典_model_entrypoints = {}  # mapping of model names to entrypoint fns</p>
<p>包含了所有的模型名称和他们各自的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">is_model</span>(<span class="params">model_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot; Check if a model name exists</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> model_name <span class="keyword">in</span> _model_entrypoints</span><br></pre></td></tr></table></figure>

<p>model_entrypoint 函数从 <code>_model_entrypoints</code> 内部得到模型的构造函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">model_entrypoint</span>(<span class="params">model_name</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Fetch a model entrypoint for specified model name</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> _model_entrypoints[model_name]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(timm.models.registry._model_entrypoints)</span><br><span class="line"><span class="built_in">print</span>(timm.models.registry._model_entrypoints.keys())</span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">输出是下面这样子的，太多了，只贴出来一点</span></span><br><span class="line"><span class="string">&#123;&#x27;vit_tiny_patch16_224&#x27;: &lt;function vit_tiny_patch16_224 at 0x000002450498A048&gt;, &#x27;vit_tiny_patch16_384&#x27;: &lt;function vit_tiny_patch16_384 at 0x000002450498A0D0&gt;, &#x27;vit_small_patch32_224&#x27;: &lt;function vit_small_patch32_224 at 0x000002450498A158&gt;, &#x27;vit_small_patch32_384&#x27;: &lt;function vit_small_patch32_384 at 0x000002450498A1E0&gt;&#125;</span></span><br><span class="line"><span class="string">可以看到是个字典，key是模型名称，value是模型的构造函数，</span></span><br><span class="line"><span class="string">以vit为例，vit_base_patch16_224模型的构造函数，他存在于timm.models.vision_trasnformer.py</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">create_fn = model_entrypoint(model_name)</span></span><br><span class="line"><span class="string">#以model_name =  &#x27;xception71&#x27; 为例</span></span><br><span class="line"><span class="string">#&lt;function xception71 at 0x7fc0cba0eca0&gt;</span></span><br><span class="line"><span class="string">#&lt;function timm.models.xception_aligned.xception71(pretrained=False, **kwargs)&gt;</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>

<h2 id="timm-models-vision-trasnformer模块"><a href="#timm-models-vision-trasnformer模块" class="headerlink" title="timm.models.vision_trasnformer模块"></a>timm.models.vision_trasnformer模块</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">@register_model</span><br><span class="line">def vit_base_patch16_224(pretrained=False, **kwargs):</span><br><span class="line">    &quot;&quot;&quot; ViT-Base (ViT-B/16) from original paper (https://arxiv.org/abs/2010.11929).</span><br><span class="line">    ImageNet-1k weights fine-tuned from in21k @ 224x224, source https://github.com/google-research/vision_transformer.</span><br><span class="line">    &quot;&quot;&quot;</span><br><span class="line">    model_kwargs = dict(patch_size=16, embed_dim=768, depth=12, num_heads=12, **kwargs)</span><br><span class="line">    model = _create_vision_transformer(&#x27;vit_base_patch16_224&#x27;, pretrained=pretrained, **model_kwargs)</span><br><span class="line">    return model</span><br></pre></td></tr></table></figure>

<p>注释写了出处，模型的原论文</p>
<p>model_kwargs把我这个模型的独特参数和传进来的**kwargs一起打包成参数字典，传给<code>_create_vision_transformer</code>函数</p>
<p>这个函数写在文件的开头，vit的模型都是他生成的，区别就是架构参数不一样，比如层数，注意力头数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_create_vision_transformer</span>(<span class="params">variant, pretrained=<span class="literal">False</span>, **kwargs</span>):</span></span><br><span class="line">    <span class="keyword">if</span> kwargs.get(<span class="string">&#x27;features_only&#x27;</span>, <span class="literal">None</span>):</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">&#x27;features_only not implemented for Vision Transformer models.&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="string">&#x27;flexi&#x27;</span> <span class="keyword">in</span> variant:</span><br><span class="line">        <span class="comment"># FIXME Google FlexiViT pretrained models have a strong preference for bilinear patch / embed</span></span><br><span class="line">        <span class="comment"># interpolation, other pretrained models resize better w/ anti-aliased bicubic interpolation.</span></span><br><span class="line">        _filter_fn = partial(checkpoint_filter_fn, interpolation=<span class="string">&#x27;bilinear&#x27;</span>, antialias=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        _filter_fn = checkpoint_filter_fn</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> build_model_with_cfg(</span><br><span class="line">        VisionTransformer, variant, pretrained,</span><br><span class="line">        pretrained_filter_fn=_filter_fn,</span><br><span class="line">        **kwargs,</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>

<p>挖坑提高，星星是什么</p>
<p>variant 变种，就是模型的名字，比如在vit类上生成的vit-base</p>
<p>函数之中，会再调用 build_model_with_cfg 函数并将一个构造器类 VisionTransformer、变量名 resnet34、一个 default_cfg 和一些 **kwargs 传入其中。</p>
<p> **kwargs就是之前打包的模型参数和  **kwargs</p>
<h2 id="build-model-with-cfg"><a href="#build-model-with-cfg" class="headerlink" title="build_model_with_cfg"></a>build_model_with_cfg</h2><p>这个 build_model_with_cfg 函数负责：</p>
<ul>
<li>真正地实例化一个模型类来创建一个模型</li>
<li>若 pruned=True，对模型进行剪枝</li>
<li>若 pretrained=True，加载预训练模型参数</li>
<li>若 features_only=True，将模型转换为特征提取器</li>
</ul>
<h2 id="register-model装饰器"><a href="#register-model装饰器" class="headerlink" title="register_model装饰器"></a>register_model装饰器</h2><p>以上就是读者在使用 timm 库时的基本方法，其实到这里你应该已经能够使用它训练自己的分类模型了。但是如果还想进一步搞清楚它的框架原理，并在它的基础上做修改，本节可能会帮到你。</p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/361837010">https://zhuanlan.zhihu.com/p/361837010</a></p>
<p>但是问题来了，这个字典怎么来的，我写好代码之后一个一个添加进去的吗？（也不是不行hh</p>
<p>但是太麻烦，这时候就用到我们的 <code>register_model</code> 装饰器，它可以不断地像其中添加模型名称和它对应的构造函数，一开始的字典是空的，是装饰器自动添加的，其源码如下</p>
<p>2.5 sys.modules<br>该属性是一个字典，包含的是各种已加载的模块的模块名到模块具体位置的映射。</p>
<p>通过手动修改这个字典，可以重新加载某些模块；但要注意，切记不要大意删除了一些基本的项，否则可能会导致 Python 整个儿无法运行。</p>
<p>mod = sys.modules[fn.__module__]</p>
<p>这里我print一下fn.__module__ 应该是xxx.xxx.vit_base</p>
<p>模型名称就是vit_base</p>
<pre><code>module_name_split = fn.__module__.split(&#39;.&#39;)
module_name = module_name_split[-1] if len(module_name_split) else &#39;&#39;
</code></pre>
<p>还是打印一下，我有点分不清module_name和model_name</p>
<p>  _model_to_module[model_name] = module_name</p>
<p>  _module_to_models[module_name].add(model_name)</p>
<details>
  <summary>展开查看</summary>
  <pre><code>
def register_model(fn):
    # lookup containing module
    mod = sys.modules[fn.__module__]
    module_name_split = fn.__module__.split('.')
    module_name = module_name_split[-1] if len(module_name_split) else ''
    # add model to __all__ in module
    model_name = fn.__name__
    if hasattr(mod, '__all__'):
        mod.__all__.append(model_name)
    else:
        mod.__all__ = [model_name]
    # add entries to registry dict/sets
    _model_entrypoints[model_name] = fn
    _model_to_module[model_name] = module_name
    _module_to_models[module_name].add(model_name)
    has_pretrained = False  # check if model has a pretrained url to allow filtering on this
    if hasattr(mod, 'default_cfgs') and model_name in mod.default_cfgs:
        # this will catch all models that have entrypoint matching cfg key, but miss any aliasing
        # entrypoints or non-matching combos
        has_pretrained = 'url' in mod.default_cfgs[model_name] and 'http' in mod.default_cfgs[model_name]['url']
        _model_default_cfgs[model_name] = deepcopy(mod.default_cfgs[model_name])
    if has_pretrained:
        _model_has_pretrained.add(model_name)
    return fn
  </code></pre>
</details>

<p>重点是这一句</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model_name = fn.__name__</span><br><span class="line"></span><br><span class="line">_model_entrypoints[model_name] = fn</span><br></pre></td></tr></table></figure>

<p>它将给定的 <code>fn</code> 添加到 <code>_model_entrypoints</code> 其键名为 <code>fn.__name__</code></p>
<p>查看模型定义文件</p>
<p>我们会发现 timm 中的每个模型都有一个 @register_model</p>
<p>例如vit-base</p>
<p>代码很简单明了，但是我还是不懂他怎么发生作用的</p>
<p>他是什么时候开始把构造函数加进那个空字典的？</p>
<p>我要自己注册函数应该如何做？</p>
<p>挖坑拓展，import</p>
<h2 id="modelconfig"><a href="#modelconfig" class="headerlink" title="modelconfig"></a>modelconfig</h2><p>timm 中所有的模型都有一个默认的配置，包括指向它的预训练权重参数的URL、类别数、输入图像尺寸、池化尺寸等。</p>
<p>此默认配置与其他参数（如构造函数类和一些模型参数）一起传递给 <code>build_model_with_cfg</code> 函数</p>
<h1 id="下载pretrained"><a href="#下载pretrained" class="headerlink" title="下载pretrained"></a>下载pretrained</h1><h3 id="pretrained"><a href="#pretrained" class="headerlink" title="pretrained"></a>pretrained</h3><p>如果我们传入 <code>pretrained=True</code>，那么 timm 会从对应的 URL 下载模型权重参数并载入模型，只有当第一次（即本地还没有对应模型参数时）会去下载，之后会直接从本地加载模型权重参数。</p>
<p>True:timm直接根据对应的URL下载模型权重参数并加载到模型，注意，只有本地没有对应模型参数时才会下载，也就是说，通常在第一次运行时下载对应模型参数，之后会直接从本地加载模型权重参数。（报错经验：这里容易出现的报错，就是第一次下载时，可能因为网络等原因，下载不完全，因此，有时需要删除对应文件，重新下载）</p>
<p>model = timm.create_model(‘resnet34’, pretrained=True)</p>
<p>Downloading: “<a target="_blank" rel="noopener" href="https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth&quot;">https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet34-43635321.pth&quot;</a> to /home/song/.cache/torch/hub/checkpoints/resnet34-43635321.pth</p>
<h1 id="yuanli"><a href="#yuanli" class="headerlink" title="yuanli"></a>yuanli</h1><h2 id="registry"><a href="#registry" class="headerlink" title="registry"></a>registry</h2><p>在了解了 <code>create_model</code> 函数的基本使用之后，我们来深入探索一下 <code>create_model</code> 函数的源码，看一下究竟是怎样实现从模型到特征提取器的转换的</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44966641/article/details/121364784">https://blog.csdn.net/weixin_44966641/article/details/121364784</a></p>
<p>在 timm 内部，有一个字典称为 <code>_model_entrypoints</code> 包含了所有的模型名称和他们各自的函数。比如说，我们可以通过 <code>model_entrypoint</code> 函数从 <code>_model_entrypoints</code> 内部得到 <code>xception71</code> 模型的构造函数。</p>
<p>如我们所见，在 <code>timm.models.xception_aligned</code> 模块中有一个函数称为 <code>xception71</code> 。类似的，timm 中的每一个模型都有着一个这样的构造函数。事实上，内部的 <code>_model_entrypoints</code> 字典大概长这个样子：</p>
<details style='background-color:#f9f2f4'> <summary><font color='#c7254e' size='3px'> view code</font></summary>  不适合放代码，都锁在一起了  </details>


<p>在 timm 对应的模块中，每个模型都有一个构造器。比如说 ResNets 系列模型被定义在 <code>timm.models.resnet</code> 模块中。因此，实际上我们有两种方式来创建一个 <code>resnet34</code> 模型</p>
<p>import timm from timm.models.resnet import resnet34 <em># 使用 create_model</em> m = timm.create_model(‘resnet34’) <em># 直接调用构造函数</em> m = resnet34()</p>
<p>但使用上，我们无须调用构造函数。所用模型都可以通过 <code>create_model</code> 函数来将创建。</p>
<h1 id="画图，话一个相互之间构成联系的图"><a href="#画图，话一个相互之间构成联系的图" class="headerlink" title="画图，话一个相互之间构成联系的图"></a>画图，话一个相互之间构成联系的图</h1><p>whiteboard</p>
<p>boardmix</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2023/03/18/5.model/" data-id="clfcr7mm00000c4u5hccu61hz" data-title="" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-系列文档汇总" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/11/18/%E7%B3%BB%E5%88%97%E6%96%87%E6%A1%A3%E6%B1%87%E6%80%BB/" class="article-date">
  <time class="dt-published" datetime="2021-11-18T08:13:37.000Z" itemprop="datePublished">2021-11-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/11/18/%E7%B3%BB%E5%88%97%E6%96%87%E6%A1%A3%E6%B1%87%E6%80%BB/">文档汇总</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>游戏系列</p>
<p>爱好 绘画 电影 rap 魔术…</p>
<p>论文阅读</p>
<p>项目经验</p>
<p>正经人不写日记</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/11/18/%E7%B3%BB%E5%88%97%E6%96%87%E6%A1%A3%E6%B1%87%E6%80%BB/" data-id="clfcr7mmb0004c4u5budo5a77" data-title="文档汇总" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-newpapername" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/11/18/newpapername/" class="article-date">
  <time class="dt-published" datetime="2021-11-18T07:52:01.000Z" itemprop="datePublished">2021-11-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/11/18/newpapername/">newpapername</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>练习</p>
<p>机制：推箱子-黑洞吸取</p>
<p>光透过绿色滤镜变成绿色</p>
<h1 id="doom"><a href="#doom" class="headerlink" title="doom"></a>doom</h1><img src="C:\Users\l\AppData\Roaming\Typora\typora-user-images\image-20211007113824448.png" alt="image-20211007113824448" style="zoom:50%;" />



<h2 id="整体感受-：doom-老师傅射爆"><a href="#整体感受-：doom-老师傅射爆" class="headerlink" title="整体感受 ：doom 老师傅射爆"></a>整体感受 ：doom 老师傅射爆</h2><h2 id="游戏介绍：历史地位第一款fps？"><a href="#游戏介绍：历史地位第一款fps？" class="headerlink" title="游戏介绍：历史地位第一款fps？"></a>游戏介绍：历史地位第一款fps？</h2><h2 id="游戏机制分析"><a href="#游戏机制分析" class="headerlink" title="游戏机制分析"></a>游戏机制分析</h2><h3 id="从几个方面-3c-ui-特殊机制-地图设计-故事-画面视角"><a href="#从几个方面-3c-ui-特殊机制-地图设计-故事-画面视角" class="headerlink" title="从几个方面 3c/ui/特殊机制/地图设计/故事/画面视角"></a>从几个方面 3c/ui/特殊机制/地图设计/故事/画面视角</h3><h1 id="不思议的皇冠"><a href="#不思议的皇冠" class="headerlink" title="不思议的皇冠"></a>不思议的皇冠</h1><img src="C:\Users\l\AppData\Roaming\Typora\typora-user-images\image-20211007114518629.png" alt="image-20211007114518629" style="zoom:50%;" />



<h2 id="整体感受-：不思议迷宫-宝可梦也有"><a href="#整体感受-：不思议迷宫-宝可梦也有" class="headerlink" title="整体感受 ：不思议迷宫 宝可梦也有"></a>整体感受 ：不思议迷宫 宝可梦也有</h2><h2 id="游戏介绍：回合制变种（捋一下回合制发展"><a href="#游戏介绍：回合制变种（捋一下回合制发展" class="headerlink" title="游戏介绍：回合制变种（捋一下回合制发展"></a>游戏介绍：回合制变种（捋一下回合制发展</h2><h2 id="游戏机制分析-1"><a href="#游戏机制分析-1" class="headerlink" title="游戏机制分析"></a>游戏机制分析</h2><h3 id="从几个方面-3c-ui-特殊机制-地图设计-故事-画面视角-1"><a href="#从几个方面-3c-ui-特殊机制-地图设计-故事-画面视角-1" class="headerlink" title="从几个方面 3c/ui/特殊机制/地图设计/故事/画面视角"></a>从几个方面 3c/ui/特殊机制/地图设计/故事/画面视角</h3><h1 id="psychonauts"><a href="#psychonauts" class="headerlink" title="psychonauts"></a>psychonauts</h1><img src="C:\Users\l\AppData\Roaming\Typora\typora-user-images\image-20211007114218717.png" alt="image-20211007114218717" style="zoom:50%;" />



<h1 id="超域现空间"><a href="#超域现空间" class="headerlink" title="超域现空间"></a>超域现空间</h1><img src="C:\Users\l\AppData\Roaming\Typora\typora-user-images\image-20211007114313417.png" alt="image-20211007114313417" style="zoom:50%;" />

<h1 id="灵魂摆渡"><a href="#灵魂摆渡" class="headerlink" title="灵魂摆渡"></a>灵魂摆渡</h1><img src="C:\Users\l\AppData\Roaming\Typora\typora-user-images\image-20211007114353805.png" alt="image-20211007114353805" style="zoom:50%;" />

<h1 id="art"><a href="#art" class="headerlink" title="art"></a>art</h1><img src="C:\Users\l\AppData\Roaming\Typora\typora-user-images\image-20211007114413064.png" alt="image-20211007114413064" style="zoom:50%;" />





<h1 id="深海迷航"><a href="#深海迷航" class="headerlink" title="深海迷航"></a>深海迷航</h1><img src="C:\Users\l\AppData\Roaming\Typora\typora-user-images\image-20211007114435132.png" alt="image-20211007114435132" style="zoom:50%;" />




      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/11/18/newpapername/" data-id="clfcr7mm70003c4u5hbnr0l8q" data-title="newpapername" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  
    <article id="post-hello-world" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/11/18/hello-world/" class="article-date">
  <time class="dt-published" datetime="2021-11-18T05:22:57.248Z" itemprop="datePublished">2021-11-18</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="p-name article-title" href="/2021/11/18/hello-world/">Hello World</a>
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>Welcome to <a target="_blank" rel="noopener" href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a target="_blank" rel="noopener" href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a target="_blank" rel="noopener" href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a target="_blank" rel="noopener" href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a target="_blank" rel="noopener" href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/11/18/hello-world/" data-id="clfcr7mm60002c4u5a8o09ypp" data-title="Hello World" class="article-share-link">Share</a>
      
      
      
    </footer>
  </div>
  
</article>



  


</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/03/">March 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/11/">November 2021</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2023/03/18/5/">5</a>
          </li>
        
          <li>
            <a href="/2023/03/18/5.model/">(no title)</a>
          </li>
        
          <li>
            <a href="/2021/11/18/%E7%B3%BB%E5%88%97%E6%96%87%E6%A1%A3%E6%B1%87%E6%80%BB/">文档汇总</a>
          </li>
        
          <li>
            <a href="/2021/11/18/newpapername/">newpapername</a>
          </li>
        
          <li>
            <a href="/2021/11/18/hello-world/">Hello World</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 John Doe<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>